%!TEX root = ../main.tex
\chapter*{Résumé}

Dans ce mémoire de thèse CIFRE, nous abordons deux thèmes, le clustering de Gaussiennes en haute dimension d'une part et l'estimation de densités agrégées d'une autre part. Le premier chapitre (\ref{chap:intro}) est une introduction au clustering. Nous y présentons différentes méthodes répandues et nous nous concentrons sur le principal outil de notre travail qui est le mélange de Gaussiennes. Nous abordons aussi les problèmes inhérents à l'estimation en haute dimension (section \ref{sec:hd_curse}) et la difficulté d'estimer le nombre de clusters (section \ref{sec:estim_nb_clusters}).  Nous rappelons brièvement ici les notions abordées dans ce manuscrit. 

Considérons une loi mélange de $K$ Gaussiennes qui admet une densité $f$ par rapport à la mesure de Lebesgue. Notons $\bmu_i$ et $\bSigma_i$ respectivement la moyenne et la variance de la $i$-ème composante Gaussienne dans $\RR^p$. Alors, la densité $f$ peut s'écrire pour $x\in\RR^p$: $f(x)= \sum_{k=1}^K\pi_k\varphi_{\bmu_k,\bSigma_k}(x)$, où $\bpi=(\pi_1,\dots,\pi_K)$ est le vecteur de poids du mélange dans $[0,1]^K$ qui vérifie $\sum_{k=1}^K\pi_k=1$, et $\varphi_{\bmu_k,\bSigma_k}$ est la densité de la $k$-ième Gaussienne. Supposons $\bX_1,\dots,\bX_N$, $N$ variables aléatoires i.i.d. dans $\RR^p$, l'approche courante pour estimer les paramètres du mélange est d'utiliser l'estimateur du maximum de vraisemblance: $\hat\btheta = \argmax_{\btheta}\textnormal{P}_{\btheta}(\bX_1,\dots,\bX_N)$. Ce problème n'étant pas convexe, une approche est d'utiliser la procédure itérative ``Expectation-Maximization'' (EM) décrite dans la section \ref{sec:EM_algo}. Malheureusement, cette méthode s’avère inefficace dans le régime de la haute dimension. Par ailleurs, il est nécessaire de connaître le nombre de clusters afin de l'utiliser.

Le chapitre \ref{chap:contributions} présente trois méthodes que nous avons développé pour tenter de résoudre les problèmes décrits précédemment. Les travaux qui y sont présentés n'ont pas fait l'objet de recherches approfondies pour diverses raisons. La première méthode que l'on pourrait appeler ``lasso graphique sur des mélanges de Gaussiennes'' consiste à estimer l'inverse des matrices de covariance $\bSigma$ (section \ref{chapgraphlasso}) dans l'hypothèse où celles ci sont parcimonieuses. Nous adaptons la méthode du lasso graphique de \citep{glasso07} sur une composante dans le cas d'un mélange et nous évaluons expérimentalement cette méthode. Les deux autres méthodes abordent le problème d'estimation du nombre de clusters dans le mélange. La première est une estimation pénalisée de la matrice des posteriors $\bTau\in\RR^{N\times K}$ dont la composante $(i,j)$ est définie par $\tau_{ij}=\textnormal{P}(\bZ=j|\bX=\bx_i)$ où $\bZ$ est une variable latente à valeurs dans $[K]$. Celle ci indique le cluster à partir duquel l'observation $\bx$ est tirée. Malheureusement, cette méthode s'est avérée trop coûteuse en complexité (section \ref{sec:tau_pen_estim}). Enfin, la deuxième méthode considérée consiste à pénaliser le vecteur de poids $\bpi$ afin de le rendre parcimonieux et montre des résultats prometteurs (section \ref{sparse_weight_vect_estim}).

Dans le chapitre \ref{chap:kl_aggreg}, nous étudions l'estimateur du maximum de vraisemblance d'une densité de $N$ observations i.i.d. sous l’hypothèse qu'elle est bien approximée par un mélange de plusieurs densités. Nous nous intéressons aux performances de l'estimateur par rapport à la perte de Kullback-Leibler. Nous établissons des bornes de risque sous la forme d'inégalités d'oracle exactes, que ce soit en probabilité ou en espérance. Nous démontrons à travers ces bornes que, dans le cas du problème d’agrégation convexe, l'estimateur du maximum de vraisemblance atteint la vitesse optimale $((\log K)/n)^{\nicefrac12}$, à une différence logarithmique près, lorsque le nombre de composant est plus grand que $n^{\nicefrac12}$. Plus important, sous l’hypothèse supplémentaire que la matrice de Gram des composantes du dictionnaire satisfait la condition de compatibilité, les inégalités d'oracles obtenues donnent la vitesse optimale dans le scénario parcimonieux. En d'autres termes, si le vecteur de poids est (presque) $D$-parcimonieux, nous obtenons une vitesse $(D\log K)/n$. En compléments de ces inégalités d'oracle, nous introduisons la notion d’agrégation (presque)-$D$-parcimonieuse et établissons pour ce type d’agrégation les bornes inférieures correspondantes.

Enfin, dans le chapitre \ref{chap:exp_kl_aggreg}, nous proposons un algorithme qui réalise l'agrégation en Kullback-Leibler de composantes d'un dictionnaire telle qu'étudiée dans le chapitre \ref{chap:kl_aggreg}. Nous comparons sa performance avec différentes méthodes: l'estimateur de densité à noyaux, l'estimateur ``Adaptive Dantzig'', l'estimateur SPADES et EM avec le critère BIC. Nous proposons ensuite une méthode pour construire le dictionnaire de densités.
