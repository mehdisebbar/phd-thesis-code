%!TEX root = ../main.tex
\chapter*{Résumé}

Dans ce mémoire de thèse\footnote{Thèse effectué dans le cadre d'une convention CIFRE avec l'entreprise ARTEFACT.}, nous abordons deux thèmes, le clustering en haute dimension d'une part et l'estimation de densités de mélange d'autre part. Le premier chapitre est une introduction au clustering. Nous y présentons différentes méthodes répandues et nous nous concentrons sur un des principaux modèles de notre travail qui est le mélange de Gaussiennes. Nous abordons aussi les problèmes inhérents à l'estimation en haute dimension (Section \ref{sec:hd_curse}) et la difficulté d'estimer le nombre de clusters (Section \ref{sec:estim_nb_clusters}).  Nous exposons brièvement ici les notions abordées dans ce manuscrit.

Considérons une loi mélange de $K$ Gaussiennes dans $\RR^p$ et notons $f$ sa densité par rapport à la mesure de Lebesgue. Notons $\bmu_k$ et $\bSigma_k$ respectivement la moyenne et la variance de la $k$-ème composante Gaussienne. Alors, la densité $f$ peut s'écrire pour $x\in\RR^p$: $f(x)= \sum_{k=1}^K\pi_k\varphi_{\bmu_k,\bSigma_k}(x)$, où $\bpi=(\pi_1,\dots,\pi_K)$ est le vecteur de poids du mélange dans $[0,1]^K$ qui vérifie $\sum_{k=1}^K\pi_k=1$, et $\varphi_{\bmu_k,\bSigma_k}$ est la densité de la $k$-ième Gaussienne. Soient $\bX_1,\dots,\bX_n$, $n$ variables aléatoires i.i.d. dans $\RR^p$. Une des approches courantes pour estimer les paramètres du mélange est d'utiliser l'estimateur du maximum de vraisemblance: $\hat\btheta = \argmax_{\btheta}\textnormal{P}_{\btheta}(\bX_1,\dots,\bX_n)$. Dans la dernière formule, la notation $\theta$ désigne l'ensemble des paramètres décrivant une loi de mélange (moyennes, matrices de covariances et poids des composantes). Ce problème n'étant pas convexe, on ne peut garantir la convergence des méthodes classiques comme la descente de gradient ou l'algorithme de Newton. Cependant, en exploitant la biconvexité de la log-vraisemblance négative, on peut utiliser la procédure itérative ``Expectation-Maximization'' (EM) décrite dans la Section \ref{sec:EM_algo}. Malheureusement, cette méthode n'est pas bien adaptée pour relever les défis posés par la grande dimension. Par ailleurs, il est nécessaire de connaître le nombre de clusters afin de l'utiliser.

Le Chapitre \ref{chap:contributions} présente trois méthodes que nous avons développées pour tenter de résoudre les problèmes décrits précédemment. Les travaux qui y sont exposés n'ont pas fait l'objet de recherches approfondies pour diverses raisons. La première méthode que l'on pourrait appeler ``lasso graphique sur des mélanges de Gaussiennes'' consiste à estimer les matrices inverses des matrices de covariance $\bSigma$ (Section \ref{chapgraphlasso}) dans l'hypothèse où celles-ci sont parcimonieuses. Nous adaptons la méthode du lasso graphique de \citep{glasso07} sur une composante dans le cas d'un mélange et nous évaluons expérimentalement cette méthode. Les deux autres méthodes abordent le problème d'estimation du nombre de clusters dans le mélange. La première est une estimation pénalisée de la matrice des probabilités postérieures $\bTau\in\RR^{n\times K}$ dont la composante $(i,j)$ est la probabilité que la $i$-ème observation soit dans le $j$-ème cluster. Malheureusement, cette méthode s'est avérée trop coûteuse en complexité (Section \ref{sec:tau_pen_estim}). Enfin, la deuxième méthode considérée consiste à pénaliser le vecteur de poids $\bpi$ afin de le rendre parcimonieux. Cette méthode montre des résultats prometteurs (Section \ref{sparse_weight_vect_estim}).

Dans le Chapitre \ref{chap:kl_aggreg}, nous étudions l'estimateur du maximum de vraisemblance d'une densité de $n$ observations i.i.d. sous l’hypothèse qu'elle est bien approximée par un mélange de plusieurs densités données. Nous nous intéressons aux performances de l'estimateur par rapport à la perte de Kullback-Leibler. Nous établissons des bornes de risque sous la forme d'inégalités d'oracle exactes, que ce soit en probabilité ou en espérance. Nous démontrons à travers ces bornes que, dans le cas du problème d’agrégation convexe, l'estimateur du maximum de vraisemblance atteint la vitesse $((\log K)/n)^{\nicefrac12}$, qui est optimale à un terme logarithmique près, lorsque le nombre de composant est plus grand que $n^{\nicefrac12}$. Plus important, sous l’hypothèse supplémentaire que la matrice de Gram des composantes du dictionnaire satisfait la condition de compatibilité, les inégalités d'oracles obtenues donnent la vitesse optimale dans le scénario parcimonieux. En d'autres termes, si le vecteur de poids est (presque) $D$-parcimonieux, nous obtenons une vitesse $(D\log K)/n$. En complément de ces inégalités d'oracle, nous introduisons la notion d’agrégation (presque)-$D$-parcimonieuse et établissons pour ce type d’agrégation les bornes inférieures correspondantes.

Enfin, dans le Chapitre \ref{chap:exp_kl_aggreg}, nous proposons un algorithme qui réalise l'agrégation en Kullback-Leibler de composantes d'un dictionnaire telle qu'étudiée dans le Chapitre \ref{chap:kl_aggreg}. Nous comparons sa performance avec différentes méthodes: l'estimateur de densité à noyaux, l'estimateur ``Adaptive Dantzig'', l'estimateur SPADES et EM avec le critère BIC. Nous proposons ensuite une méthode pour construire le dictionnaire de densités et l’étudions de manière numérique.
